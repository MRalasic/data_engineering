{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In the project we will create a ETL pipeline to  Data Lake using US I94 Immigration data. We will extract, process, clean and store data that will later on be used to analyse tourist and immigration flow to US through different airports. We could set some expectations on the inflow of the tourist, as well the necessary exchange currencies for tourists. We could extract seasonalities and prepare accordingly.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import isnan, when, count, col, upper, split, year, month, avg, isnull\n",
    "from mappings import country_codes_mapping, i94visa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We will create a fact table and dimension tables using five sources described below. We will use Apache Sparke to explore, load, transform and save data to parquet files. We will create a fact table that would allow analyzing the arrivals and seasonality of arrivals in the U.S. cities. This fact table could be used by some turist officers, or companies offering services to tourists iin the peak of the season.\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "The following datasets are included in the project workspace. We purposely did not include a lot of detail about the data and instead point you to the sources. This is to help you get experience doing a self-guided project and researching the data yourself. If something about the data is unclear, make an assumption, document it, and move on. Feel free to enrich your project by gathering and including additional data sources.\n",
    "\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "- World Temperature Data: This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/selfishgene/historical-hourly-weather-data).\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "- Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "- Countries Table: This is a simple table of country codes and additional information about countries. It comes from [here](https://datahub.io/JohnSnowLabs/iso-3166-country-codes-itu-dialing-codes-iso-4217-currency-codes/r/iso-3166-country-codes-itu-dialing-codes-iso-4217-currency-codes-csv.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://datahub.io/JohnSnowLabs/iso-3166-country-codes-itu-dialing-codes-iso-4217-currency-codes/r/iso-3166-country-codes-itu-dialing-codes-iso-4217-currency-codes-csv.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_data = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(\"us-cities-demographics.csv\")\n",
    "airport_data = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "country_data = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"iso-3166-country-codes-itu-dialing-codes-iso-4217-currency-codes-csv.csv\")\n",
    "city_data = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(\"city_codes.csv\")\n",
    "temperature_data = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Silver Spring', State='Maryland', Median Age='33.8', Male Population='40601', Female Population='41862', Total Population='82463', Number of Veterans='1562', Foreign-born='30908', Average Household Size='2.6', State Code='MD', Race='Hispanic or Latino', Count='25924')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Country_Name=None, Official_Name_English='Channel Islands', ISO3166_1_Alpha_2=None, ISO3166_1_Alpha_3=None, M49='830', ITU=None, MARC=None, WMO=None, DS=None, Dial=None, FIFA=None, FIPS=None, GAUL=None, IOC=None, ISO4217_Currency_Alphabetic_Code=None, ISO4217_Currency_Country_Name=None, ISO4217_Currency_Minor_Unit=None, ISO4217_Currency_Name=None, ISO4217_Currency_Numeric_Code=None, Is_Independent=None, Capital=None, Continent=None, TLD=None, Languages=None, Geo_Name_ID=None, EDGAR=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(city_code='ALC', city_name='ALCAN')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dt='1743-11-01', AverageTemperature='6.068', AverageTemperatureUncertainty='1.7369999999999999', City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMMIGRATION_DATA_FILENAMES = [\n",
    "    'i94_jan16_sub.sas7bdat',\n",
    "    'i94_feb16_sub.sas7bdat',\n",
    "    'i94_mar16_sub.sas7bdat',\n",
    "    'i94_apr16_sub.sas7bdat',\n",
    "    'i94_may16_sub.sas7bdat',\n",
    "    'i94_jun16_sub.sas7bdat',\n",
    "    'i94_jul16_sub.sas7bdat',\n",
    "    'i94_aug16_sub.sas7bdat',\n",
    "    'i94_sep16_sub.sas7bdat',\n",
    "    'i94_oct16_sub.sas7bdat',\n",
    "    'i94_nov16_sub.sas7bdat',\n",
    "    'i94_dec16_sub.sas7bdat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file_name in enumerate(IMMIGRATION_DATA_FILENAMES):\n",
    "    if i==0:\n",
    "        immigration_data = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/' + file_name)\n",
    "        if file_name=='i94_jun16_sub.sas7bdat':\n",
    "            immigration_data = immigration_data.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa')\n",
    "    else:\n",
    "        immigration_data_subset = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/' + file_name)\n",
    "        if file_name=='i94_jun16_sub.sas7bdat':\n",
    "            immigration_data_subset = immigration_data_subset.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa')\n",
    "        immigration_data.union(immigration_data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=7.0, i94yr=2016.0, i94mon=1.0, i94cit=101.0, i94res=101.0, i94port='BOS', arrdate=20465.0, i94mode=1.0, i94addr='MA', depdate=None, i94bir=20.0, i94visa=3.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu=None, matflag=None, biryear=1996.0, dtaddto='D/S', gender='M', insnum=None, airline='LH', admnum=346608285.0, fltno='424', visatype='F1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data.write.parquet(\"sas_data\")\n",
    "immigration_data = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "In this step we will try to identify data quality issues, like missing values, duplicate data, etc. For that purpouse we will check for NaN and null values in our dataframes, and compare duplicate and deduplicated counts. In the future we could create a function that would check out the check we did in all the sample datasets, but for the sake of show and tell we will keep this in iPython cells. We will explore our datasets seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Airport dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " ident        | 00A                  \n",
      " type         | heliport             \n",
      " name         | Total Rf Heliport    \n",
      " elevation_ft | 11                   \n",
      " continent    | NA                   \n",
      " iso_country  | US                   \n",
      " iso_region   | US-PA                \n",
      " municipality | Bensalem             \n",
      " gps_code     | 00A                  \n",
      " iata_code    | null                 \n",
      " local_code   | 00A                  \n",
      " coordinates  | -74.9336013793945... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "| small_airport|33965|\n",
      "|      heliport|11287|\n",
      "|medium_airport| 4550|\n",
      "|        closed| 3606|\n",
      "| seaplane_base| 1016|\n",
      "| large_airport|  627|\n",
      "|   balloonport|   24|\n",
      "+--------------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|iso_country|count|\n",
      "+-----------+-----+\n",
      "|         US|22757|\n",
      "|         BR| 4334|\n",
      "|         CA| 2784|\n",
      "|         AU| 1963|\n",
      "|         KR| 1376|\n",
      "|         MX| 1181|\n",
      "|         RU| 1040|\n",
      "|         DE|  947|\n",
      "|         GB|  911|\n",
      "|         FR|  850|\n",
      "|         AR|  848|\n",
      "|         CO|  706|\n",
      "|         IT|  671|\n",
      "|         PG|  593|\n",
      "|         VE|  592|\n",
      "|         ZA|  489|\n",
      "|         CL|  474|\n",
      "|         ID|  470|\n",
      "|         ES|  416|\n",
      "|         CN|  404|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----+\n",
      "|iso_region|count|\n",
      "+----------+-----+\n",
      "|     US-TX| 2277|\n",
      "|     US-CA| 1088|\n",
      "|     US-FL|  967|\n",
      "|     US-PA|  918|\n",
      "|     BR-SP|  907|\n",
      "|     US-IL|  902|\n",
      "|     US-AK|  829|\n",
      "|     US-OH|  799|\n",
      "|    GB-ENG|  726|\n",
      "|     US-IN|  697|\n",
      "|     CA-ON|  695|\n",
      "|     US-NY|  668|\n",
      "|     BR-MT|  635|\n",
      "|     US-WI|  624|\n",
      "|     US-LA|  592|\n",
      "|     US-WA|  578|\n",
      "|     US-MO|  578|\n",
      "|     US-MN|  569|\n",
      "|     US-MI|  549|\n",
      "|     US-OK|  537|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|       NA|27719|\n",
      "|       EU| 7840|\n",
      "|       SA| 7709|\n",
      "|       AS| 5350|\n",
      "|       AF| 3362|\n",
      "|       OC| 3067|\n",
      "|       AN|   28|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|iata_code|count|\n",
      "+---------+-----+\n",
      "|     null|45886|\n",
      "|        0|   80|\n",
      "|      OHE|    3|\n",
      "|      PRI|    3|\n",
      "|      AUS|    2|\n",
      "|      IZA|    2|\n",
      "|      NWT|    2|\n",
      "|      CMN|    2|\n",
      "|      NKB|    2|\n",
      "|      SHO|    2|\n",
      "|      MNW|    2|\n",
      "|      CLG|    2|\n",
      "|      PCO|    2|\n",
      "|      TFY|    2|\n",
      "|      ULG|    2|\n",
      "|      RTI|    2|\n",
      "|      MXR|    2|\n",
      "|      RCH|    2|\n",
      "|      MUP|    2|\n",
      "|      SVD|    2|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.groupBy(\"type\").count().orderBy(\"count\", ascending=False).show()\n",
    "airport_data.groupBy(\"iso_country\").count().orderBy(\"count\", ascending=False).show()\n",
    "airport_data.groupBy(\"iso_region\").count().orderBy(\"count\", ascending=False).show()\n",
    "airport_data.groupBy(\"continent\").count().orderBy(\"count\", ascending=False).show()\n",
    "airport_data.groupBy(\"iata_code\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|           0|        0|          0|         0|           0|       0|        0|         0|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|        7006|        0|          0|         0|        5676|   14045|    45886|     26389|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.select([count(when(isnan(column), column)).alias(column) for column in airport_data.columns]).show()\n",
    "airport_data.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in airport_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data doesn't contain duplicates\n"
     ]
    }
   ],
   "source": [
    "deduplicated_count = airport_data.groupBy(airport_data.columns).agg((count(\"*\")>1).cast(\"int\")).count()\n",
    "dataframe_count =  airport_data.count()\n",
    "if deduplicated_count == dataframe_count:\n",
    "    print(\"Data doesn't contain duplicates\")\n",
    "else:\n",
    "    print(\"Data contains duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------\n",
      " Country_Name                     | null            \n",
      " Official_Name_English            | Channel Islands \n",
      " ISO3166_1_Alpha_2                | null            \n",
      " ISO3166_1_Alpha_3                | null            \n",
      " M49                              | 830             \n",
      " ITU                              | null            \n",
      " MARC                             | null            \n",
      " WMO                              | null            \n",
      " DS                               | null            \n",
      " Dial                             | null            \n",
      " FIFA                             | null            \n",
      " FIPS                             | null            \n",
      " GAUL                             | null            \n",
      " IOC                              | null            \n",
      " ISO4217_Currency_Alphabetic_Code | null            \n",
      " ISO4217_Currency_Country_Name    | null            \n",
      " ISO4217_Currency_Minor_Unit      | null            \n",
      " ISO4217_Currency_Name            | null            \n",
      " ISO4217_Currency_Numeric_Code    | null            \n",
      " Is_Independent                   | null            \n",
      " Capital                          | null            \n",
      " Continent                        | null            \n",
      " TLD                              | null            \n",
      " Languages                        | null            \n",
      " Geo_Name_ID                      | null            \n",
      " EDGAR                            | null            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Country_Name|count|\n",
      "+--------------------+-----+\n",
      "|                null|    2|\n",
      "|               Tonga|    1|\n",
      "|                Chad|    1|\n",
      "|            Anguilla|    1|\n",
      "|            Paraguay|    1|\n",
      "|              Russia|    1|\n",
      "|British Indian Oc...|    1|\n",
      "| U.S. Virgin Islands|    1|\n",
      "|               Yemen|    1|\n",
      "|Heard & McDonald ...|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|             Tokelau|    1|\n",
      "|French Southern T...|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|             Eritrea|    1|\n",
      "|              Jersey|    1|\n",
      "|         Philippines|    1|\n",
      "|            Djibouti|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------------+-----+\n",
      "|Official_Name_English|count|\n",
      "+---------------------+-----+\n",
      "|                 null|   10|\n",
      "|                Tonga|    1|\n",
      "|                 Chad|    1|\n",
      "| Micronesia (Feder...|    1|\n",
      "|             Anguilla|    1|\n",
      "|             Paraguay|    1|\n",
      "| The former Yugosl...|    1|\n",
      "|                Yemen|    1|\n",
      "|   State of Palestine|    1|\n",
      "|              Senegal|    1|\n",
      "|           Cabo Verde|    1|\n",
      "|               Sweden|    1|\n",
      "|              Tokelau|    1|\n",
      "|             Kiribati|    1|\n",
      "|    Republic of Korea|    1|\n",
      "|               Guyana|    1|\n",
      "|              Eritrea|    1|\n",
      "|               Jersey|    1|\n",
      "|          Philippines|    1|\n",
      "|             Djibouti|    1|\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+\n",
      "| Dial|count|\n",
      "+-----+-----+\n",
      "|   44|    4|\n",
      "|   61|    3|\n",
      "|  262|    3|\n",
      "|  672|    3|\n",
      "|   47|    3|\n",
      "| null|    3|\n",
      "|    1|    3|\n",
      "|  590|    3|\n",
      "|  358|    2|\n",
      "|  212|    2|\n",
      "|  599|    2|\n",
      "|    7|    2|\n",
      "|  500|    2|\n",
      "|   54|    1|\n",
      "|  351|    1|\n",
      "|  853|    1|\n",
      "|  234|    1|\n",
      "|  232|    1|\n",
      "|   51|    1|\n",
      "|290 n|    1|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-----+\n",
      "|M49|count|\n",
      "+---+-----+\n",
      "|296|    1|\n",
      "|800|    1|\n",
      "|666|    1|\n",
      "| 51|    1|\n",
      "|124|    1|\n",
      "|591|    1|\n",
      "|574|    1|\n",
      "|334|    1|\n",
      "|740|    1|\n",
      "|581|    1|\n",
      "|442|    1|\n",
      "|462|    1|\n",
      "|470|    1|\n",
      "|232|    1|\n",
      "|234|    1|\n",
      "|862|    1|\n",
      "|132|    1|\n",
      "|388|    1|\n",
      "|428|    1|\n",
      "|854|    1|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.groupBy(\"Country_Name\").count().orderBy(\"count\", ascending=False).show()\n",
    "country_data.groupBy(\"Official_Name_English\").count().orderBy(\"count\", ascending=False).show()\n",
    "country_data.groupBy(\"Dial\").count().orderBy(\"count\", ascending=False).show()\n",
    "country_data.groupBy(\"M49\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "|Country_Name|Official_Name_English|ISO3166_1_Alpha_2|ISO3166_1_Alpha_3|M49|ITU|MARC|WMO| DS|Dial|FIFA|FIPS|GAUL|IOC|ISO4217_Currency_Alphabetic_Code|ISO4217_Currency_Country_Name|ISO4217_Currency_Minor_Unit|ISO4217_Currency_Name|ISO4217_Currency_Numeric_Code|Is_Independent|Capital|Continent|TLD|Languages|Geo_Name_ID|EDGAR|\n",
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "|           0|                    0|                0|                0|  0|  0|   0|  0|  0|   0|   0|   0|   0|  0|                               0|                            0|                          0|                    0|                            0|             0|      0|        0|  0|        0|          0|    0|\n",
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "\n",
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "|Country_Name|Official_Name_English|ISO3166_1_Alpha_2|ISO3166_1_Alpha_3|M49|ITU|MARC|WMO| DS|Dial|FIFA|FIPS|GAUL|IOC|ISO4217_Currency_Alphabetic_Code|ISO4217_Currency_Country_Name|ISO4217_Currency_Minor_Unit|ISO4217_Currency_Name|ISO4217_Currency_Numeric_Code|Is_Independent|Capital|Continent|TLD|Languages|Geo_Name_ID|EDGAR|\n",
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "|           2|                   10|                2|                2|  0| 18|   6| 34| 28|   3|  14|   3|   8| 25|                              17|                           16|                         17|                   16|                           17|             2|      8|        2|  3|        5|          2|   39|\n",
      "+------------+---------------------+-----------------+-----------------+---+---+----+---+---+----+----+----+----+---+--------------------------------+-----------------------------+---------------------------+---------------------+-----------------------------+--------------+-------+---------+---+---------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.select([count(when(isnan(column), column)).alias(column) for column in country_data.columns]).show()\n",
    "country_data.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in country_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data doesn't contain duplicates\n"
     ]
    }
   ],
   "source": [
    "deduplicated_count = country_data.groupBy(country_data.columns).agg((count(\"*\")>1).cast(\"int\")).count()\n",
    "dataframe_count =  country_data.count()\n",
    "if deduplicated_count == dataframe_count:\n",
    "    print(\"Data doesn't contain duplicates\")\n",
    "else:\n",
    "    print(\"Data contains duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demographic dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " City                   | Silver Spring      \n",
      " State                  | Maryland           \n",
      " Median Age             | 33.8               \n",
      " Male Population        | 40601              \n",
      " Female Population      | 41862              \n",
      " Total Population       | 82463              \n",
      " Number of Veterans     | 1562               \n",
      " Foreign-born           | 30908              \n",
      " Average Household Size | 2.6                \n",
      " State Code             | MD                 \n",
      " Race                   | Hispanic or Latino \n",
      " Count                  | 25924              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        City|count|\n",
      "+------------+-----+\n",
      "| Springfield|   15|\n",
      "| Bloomington|   15|\n",
      "|    Columbia|   15|\n",
      "|     Norwalk|   10|\n",
      "|Jacksonville|   10|\n",
      "|      Peoria|   10|\n",
      "|      Albany|   10|\n",
      "|    Pasadena|   10|\n",
      "|     Jackson|   10|\n",
      "| Kansas City|   10|\n",
      "|Fayetteville|   10|\n",
      "|    Columbus|   10|\n",
      "| Westminster|   10|\n",
      "|  Wilmington|   10|\n",
      "|    Lakewood|   10|\n",
      "|   Arlington|   10|\n",
      "|   Rochester|   10|\n",
      "|       Allen|   10|\n",
      "|    Portland|   10|\n",
      "|      Aurora|   10|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|         State|count|\n",
      "+--------------+-----+\n",
      "|    California|  676|\n",
      "|         Texas|  273|\n",
      "|       Florida|  222|\n",
      "|      Illinois|   91|\n",
      "|    Washington|   85|\n",
      "|       Arizona|   80|\n",
      "|      Colorado|   80|\n",
      "|      Michigan|   79|\n",
      "|North Carolina|   70|\n",
      "|      Virginia|   70|\n",
      "| Massachusetts|   69|\n",
      "|    New Jersey|   57|\n",
      "|       Georgia|   55|\n",
      "|     Minnesota|   54|\n",
      "|      New York|   54|\n",
      "|       Indiana|   51|\n",
      "|      Maryland|   50|\n",
      "|          Ohio|   49|\n",
      "|          Utah|   48|\n",
      "|     Wisconsin|   45|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|Male Population|count|\n",
      "+---------------+-----+\n",
      "|          33993|   10|\n",
      "|          40601|   10|\n",
      "|         319705|    5|\n",
      "|         237724|    5|\n",
      "|          77339|    5|\n",
      "|          41286|    5|\n",
      "|          35640|    5|\n",
      "|         125411|    5|\n",
      "|          70640|    5|\n",
      "|         104793|    5|\n",
      "|          63316|    5|\n",
      "|          34488|    5|\n",
      "|          50989|    5|\n",
      "|          31205|    5|\n",
      "|          71365|    5|\n",
      "|         122292|    5|\n",
      "|          34749|    5|\n",
      "|          37089|    5|\n",
      "|          32873|    5|\n",
      "|         138040|    5|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                Race|count|\n",
      "+--------------------+-----+\n",
      "|  Hispanic or Latino|  596|\n",
      "|               White|  589|\n",
      "|Black or African-...|  584|\n",
      "|               Asian|  583|\n",
      "|American Indian a...|  539|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data.groupBy(\"City\").count().orderBy(\"count\", ascending=False).show()\n",
    "demographics_data.groupBy(\"State\").count().orderBy(\"count\", ascending=False).show()\n",
    "demographics_data.groupBy(\"Male Population\").count().orderBy(\"count\", ascending=False).show()\n",
    "demographics_data.groupBy(\"Race\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              0|                0|               0|                 0|           0|                     0|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data.select([count(when(isnan(column), column)).alias(column) for column in demographics_data.columns]).show()\n",
    "demographics_data.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in demographics_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data doesn't contain duplicates\n"
     ]
    }
   ],
   "source": [
    "deduplicated_count = demographics_data.groupBy(demographics_data.columns).agg((count(\"*\")>1).cast(\"int\")).count()\n",
    "dataframe_count =  demographics_data.count()\n",
    "if deduplicated_count == dataframe_count:\n",
    "    print(\"Data doesn't contain duplicates\")\n",
    "else:\n",
    "    print(\"Data contains duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temperature dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------\n",
      " dt                            | 1743-11-01         \n",
      " AverageTemperature            | 6.068              \n",
      " AverageTemperatureUncertainty | 1.7369999999999999 \n",
      " City                          | Århus              \n",
      " Country                       | Denmark            \n",
      " Latitude                      | 57.05N             \n",
      " Longitude                     | 10.33E             \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|            City|count|\n",
      "+----------------+-----+\n",
      "|     Springfield| 9545|\n",
      "|       Worcester| 8359|\n",
      "|            León| 7469|\n",
      "|       Rongcheng| 6526|\n",
      "|           Brest| 6478|\n",
      "|        Columbus| 6478|\n",
      "|       Cambridge| 6478|\n",
      "|Saint Petersburg| 6478|\n",
      "|          London| 6478|\n",
      "|      Birmingham| 6478|\n",
      "|        Syracuse| 6478|\n",
      "|      Manchester| 6478|\n",
      "|        Santiago| 6203|\n",
      "|          Aurora| 6101|\n",
      "|      Alexandria| 5908|\n",
      "|       Arlington| 5564|\n",
      "|       Barcelona| 5516|\n",
      "|        Kingston| 5516|\n",
      "|        Valencia| 5516|\n",
      "|       Cartagena| 5516|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-------+\n",
      "|       Country|  count|\n",
      "+--------------+-------+\n",
      "|         India|1014906|\n",
      "|         China| 827802|\n",
      "| United States| 687289|\n",
      "|        Brazil| 475580|\n",
      "|        Russia| 461234|\n",
      "|         Japan| 358669|\n",
      "|     Indonesia| 323255|\n",
      "|       Germany| 262359|\n",
      "|United Kingdom| 220252|\n",
      "|        Mexico| 209560|\n",
      "|       Nigeria| 172347|\n",
      "|         Spain| 159594|\n",
      "|          Iran| 151651|\n",
      "|        Turkey| 150306|\n",
      "|      Pakistan| 139231|\n",
      "|         Italy| 136038|\n",
      "|   Philippines| 127700|\n",
      "|        Poland| 123082|\n",
      "|        France| 116604|\n",
      "|  South Africa|  94050|\n",
      "+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+------+\n",
      "|AverageTemperature| count|\n",
      "+------------------+------+\n",
      "|              null|364130|\n",
      "|            26.544|   828|\n",
      "|            26.045|   778|\n",
      "|            26.612|   765|\n",
      "|25.773000000000003|   749|\n",
      "|             26.47|   747|\n",
      "|25.843000000000004|   738|\n",
      "|            26.324|   735|\n",
      "|            25.967|   733|\n",
      "|            26.254|   729|\n",
      "|            26.326|   728|\n",
      "|            26.276|   719|\n",
      "|            25.881|   717|\n",
      "|            26.166|   715|\n",
      "|            25.914|   714|\n",
      "|             25.87|   710|\n",
      "|            25.925|   707|\n",
      "|            26.184|   707|\n",
      "|            25.698|   706|\n",
      "|            26.455|   706|\n",
      "+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------------+------+\n",
      "|AverageTemperatureUncertainty| count|\n",
      "+-----------------------------+------+\n",
      "|                         null|364130|\n",
      "|          0.28300000000000003| 13172|\n",
      "|                        0.308| 13163|\n",
      "|                        0.312| 13080|\n",
      "|          0.28800000000000003| 12995|\n",
      "|                        0.276| 12914|\n",
      "|                        0.292| 12913|\n",
      "|                        0.266| 12885|\n",
      "|                        0.315| 12884|\n",
      "|                        0.282| 12877|\n",
      "|                        0.263| 12871|\n",
      "|                        0.287| 12866|\n",
      "|                        0.306| 12851|\n",
      "|                        0.301| 12850|\n",
      "|                        0.302| 12803|\n",
      "|                        0.254| 12794|\n",
      "|                         0.28| 12790|\n",
      "|                        0.298| 12789|\n",
      "|                        0.307| 12782|\n",
      "|                        0.273| 12735|\n",
      "+-----------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.groupBy(\"City\").count().orderBy(\"count\", ascending=False).show()\n",
    "temperature_data.groupBy(\"Country\").count().orderBy(\"count\", ascending=False).show()\n",
    "temperature_data.groupBy(\"AverageTemperature\").count().orderBy(\"count\", ascending=False).show()\n",
    "temperature_data.groupBy(\"AverageTemperatureUncertainty\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|                 0|                            0|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|            364130|                       364130|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.select([count(when(isnan(column), column)).alias(column) for column in temperature_data.columns]).show()\n",
    "temperature_data.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in temperature_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data doesn't contain duplicates\n"
     ]
    }
   ],
   "source": [
    "deduplicated_count = temperature_data.groupBy(temperature_data.columns).agg((count(\"*\")>1).cast(\"int\")).count()\n",
    "dataframe_count =  temperature_data.count()\n",
    "if deduplicated_count == dataframe_count:\n",
    "    print(\"Data doesn't contain duplicates\")\n",
    "else:\n",
    "    print(\"Data contains duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immigration dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n",
      " cicid    | 7.0          \n",
      " i94yr    | 2016.0       \n",
      " i94mon   | 1.0          \n",
      " i94cit   | 101.0        \n",
      " i94res   | 101.0        \n",
      " i94port  | BOS          \n",
      " arrdate  | 20465.0      \n",
      " i94mode  | 1.0          \n",
      " i94addr  | MA           \n",
      " depdate  | null         \n",
      " i94bir   | 20.0         \n",
      " i94visa  | 3.0          \n",
      " count    | 1.0          \n",
      " dtadfile | null         \n",
      " visapost | null         \n",
      " occup    | null         \n",
      " entdepa  | T            \n",
      " entdepd  | null         \n",
      " entdepu  | null         \n",
      " matflag  | null         \n",
      " biryear  | 1996.0       \n",
      " dtaddto  | D/S          \n",
      " gender   | M            \n",
      " insnum   | null         \n",
      " airline  | LH           \n",
      " admnum   | 3.46608285E8 \n",
      " fltno    | 424          \n",
      " visatype | F1           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|i94port| count|\n",
      "+-------+------+\n",
      "|    MIA|420690|\n",
      "|    NYC|382771|\n",
      "|    LOS|339125|\n",
      "|    HHW|180162|\n",
      "|    SFR|165661|\n",
      "|    ORL|130865|\n",
      "|    CHI|120888|\n",
      "|    AGA|118170|\n",
      "|    NEW|107767|\n",
      "|    HOU|106536|\n",
      "|    ATL| 89865|\n",
      "|    DAL| 78220|\n",
      "|    FTL| 73761|\n",
      "|    WAS| 64601|\n",
      "|    BOS| 54270|\n",
      "|    LVG| 49865|\n",
      "|    SEA| 47086|\n",
      "|    SAI| 42317|\n",
      "|    DET| 40404|\n",
      "|    PHI| 21478|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------+\n",
      "|visapost|  count|\n",
      "+--------+-------+\n",
      "|    null|1386375|\n",
      "|     GUZ|  99955|\n",
      "|     SPL|  98234|\n",
      "|     MEX|  87723|\n",
      "|     BEJ|  68257|\n",
      "|     BNS|  66180|\n",
      "|     SHG|  58280|\n",
      "|     RDJ|  51798|\n",
      "|     BGT|  49826|\n",
      "|     CRS|  34854|\n",
      "|     SEO|  33793|\n",
      "|     SNJ|  29331|\n",
      "|     BMB|  23389|\n",
      "|     MDR|  22828|\n",
      "|     GDL|  21751|\n",
      "|     LMA|  19742|\n",
      "|     TLV|  18745|\n",
      "|     LND|  18539|\n",
      "|     BRA|  17625|\n",
      "|     SDO|  17177|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     M|1415249|\n",
      "|     F|1214106|\n",
      "|  null| 216929|\n",
      "|     X|   1451|\n",
      "|     U|    189|\n",
      "+------+-------+\n",
      "\n",
      "+--------+------+\n",
      "|visatype| count|\n",
      "+--------+------+\n",
      "|      B2|942869|\n",
      "|      WT|862937|\n",
      "|      F1|373311|\n",
      "|      WB|286047|\n",
      "|      B1|198414|\n",
      "|     GMT|113786|\n",
      "|      E2| 28664|\n",
      "|      CP| 20271|\n",
      "|      F2|  9675|\n",
      "|      E1|  5531|\n",
      "|       I|  3451|\n",
      "|      M1|  2098|\n",
      "|     GMB|   567|\n",
      "|      I1|   216|\n",
      "|      M2|    51|\n",
      "|     CPL|    31|\n",
      "|     SBP|     5|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.groupBy(\"i94port\").count().orderBy(\"count\", ascending=False).show()\n",
    "immigration_data.groupBy(\"visapost\").count().orderBy(\"count\", ascending=False).show()\n",
    "immigration_data.groupBy(\"gender\").count().orderBy(\"count\", ascending=False).show()\n",
    "immigration_data.groupBy(\"visatype\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|      0|      0|      0|     0|      0|    0|       0|       0|    0|      0|      0|      0|      0|      0|      0|     0|     0|      0|     0|    0|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|     60| 177129| 522612|  1190|      0|    0|   90486| 1386375|2802355|     61| 521813|2847880| 521813|   1190|    707|216929|2709236|  61279|     0|12232|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.select([count(when(isnan(column), column)).alias(column) for column in immigration_data.columns]).show()\n",
    "immigration_data.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in immigration_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data doesn't contain duplicates\n"
     ]
    }
   ],
   "source": [
    "deduplicated_count = immigration_data.groupBy(immigration_data.columns).agg((count(\"*\")>1).cast(\"int\")).count()\n",
    "dataframe_count =  immigration_data.count()\n",
    "if deduplicated_count == dataframe_count:\n",
    "    print(\"Data doesn't contain duplicates\")\n",
    "else:\n",
    "    print(\"Data contains duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Data conclusions\n",
    "We are working with five datasets: Demographic, Airport , Temperature, Country and Immigration data. None of the datasets contain duplicated data, so the datasets deduplication will not be needed. However, all datasets contain columns that contain null values, so we will need to be careful how we filter and map the data from that columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "We now proceed to cleaning the data. We will seperate the process per dataset for the sake of show and tell here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Airport dataset cleaning\n",
    "\n",
    "Since we are having a demographic dataset that contains data for the USA, we will filter the U.S. locations from the airport data. Also, we will transform city name to upper case to have a uniform case in the city name column. Furthermore, airport data contains the iso_region column that could be parsed to extract state code, and we will drop duplicates. We will filter out closed airports and ballon ports since they are probbably not used for the immigration. Last, but not least we will filter only columns for our dimensions table and give them more meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " airport_id   | K7M4                 \n",
      " city_name    | OSCEOLA              \n",
      " state_code   | AR                   \n",
      " airport_name | Osceola Municipal... \n",
      " airport_type | small_airport        \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data = airport_data.filter(airport_data[\"iso_country\"]==\"US\").withColumn(\"municipality\", upper(col(\"municipality\")))\n",
    "\n",
    "airport_data = airport_data.withColumn(\"state_code\", split(airport_data[\"iso_region\"], '-').getItem(1))\n",
    "\n",
    "airport_data = airport_data.filter(~airport_data[\"type\"].isin({\"closed\", \"balloonport\"})).drop_duplicates()\n",
    "\n",
    "airport_data = airport_data.select(\\\n",
    "                        col(\"ident\").alias(\"airport_id\"),\\\n",
    "                        upper(col(\"municipality\")).alias(\"city_name\"),\\\n",
    "                        \"state_code\",\\\n",
    "                        col(\"name\").alias(\"airport_name\"),\\\n",
    "                        col(\"type\").alias(\"airport_type\")).drop_duplicates()\n",
    "airport_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country dataset cleaning\n",
    "\n",
    "For country dataset, we will transform the country name into the uppercase for consistency, and extract and rename interesting columns. Also, we had noticed that some of the currencies were missing so we will add them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " country_name  | HEARD & MCDONALD ... \n",
      " currency_name | null                 \n",
      " capital       | null                 \n",
      " continent     | AN                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data = country_data.withColumn(\"Country_Name\", upper(col(\"Country_Name\")))\n",
    "\n",
    "\n",
    "country_data = country_data.select(\\\n",
    "                        col(\"Country_Name\").alias(\"country_name\"),\\\n",
    "                        col(\"ISO4217_Currency_Name\").alias(\"currency_name\"),\\\n",
    "                        col(\"Capital\").alias(\"capital\"),\\\n",
    "                        col(\"Continent\").alias(\"continent\")).drop_duplicates()\n",
    "\n",
    "country_data = country_data.withColumn(\"currency_name\",when(col(\"country_name\") == \"CZECH REPUBLIC\",\"Czech crown\")\\\n",
    "                        .otherwise(col(\"currency_name\")))\n",
    "\n",
    "country_data = country_data.withColumn(\"currency_name\",when(col(\"country_name\") == \"HONG KONG\",\"Hong Kong dollar\")\\\n",
    "                        .otherwise(col(\"currency_name\")))\n",
    "\n",
    "\n",
    "country_data = country_data.withColumn(\"currency_name\",when(col(\"country_name\") == \"TAIWAN\",\"Taiwan New dollar\")\\\n",
    "                        .otherwise(col(\"currency_name\")))\n",
    "\n",
    "country_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demographic dataset cleaning\n",
    "\n",
    "As in the Airport data we will transform the city name to upper case for the uniformity sake. We will give the columns more meaningfull names, cast float and integer types in columns where it makes sence, and drop eventual duplicates from the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " city_name             | ROSWELL \n",
      " state_code            | GA      \n",
      " state_name            | Georgia \n",
      " total_population      | 94496   \n",
      " male_population       | 48637   \n",
      " female_population     | 45859   \n",
      " immigrants_population | 16501   \n",
      " median_age            | 38.8    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_data = demographics_data.withColumn(\"median_age\",col(\"Median Age\").cast(\"float\"))\\\n",
    "                        .withColumn(\"City\", upper(col(\"City\")))\\\n",
    "                        .withColumn(\"Total Population\",col(\"Total Population\").cast(\"integer\"))\\\n",
    "                        .withColumn(\"Male Population\",col(\"Male Population\").cast(\"integer\"))\\\n",
    "                        .withColumn(\"Female Population\",col(\"Female Population\").cast(\"integer\"))\\\n",
    "                        .withColumn(\"Foreign-born\",col(\"Foreign-born\").cast(\"integer\"))\n",
    "\n",
    "demographics_data = demographics_data.select(\\\n",
    "    col(\"City\").alias(\"city_name\"),\\\n",
    "    col(\"State Code\").alias(\"state_code\"),\\\n",
    "    col(\"State\").alias(\"state_name\"),\\\n",
    "    col(\"Total Population\").alias(\"total_population\"),\\\n",
    "    col(\"Male Population\").alias(\"male_population\"),\\\n",
    "    col(\"Female Population\").alias(\"female_population\"),\\\n",
    "    col(\"Foreign-born\").alias(\"immigrants_population\"),\\\n",
    "    \"median_age\").drop_duplicates()\n",
    "\n",
    "demographics_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temperature dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------\n",
      " city                            | RALEIGH            \n",
      " month                           | 12                 \n",
      " average_temperature             | 4.4194827586206875 \n",
      " average_temperature_uncertainty | 1.5283026819923373 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data = temperature_data.withColumn(\"year\",year(temperature_data[\"dt\"]))\\\n",
    "                        .withColumn(\"month\",month(temperature_data[\"dt\"]))\n",
    "\n",
    "temperature_data = temperature_data.filter(temperature_data[\"country\"]==\"United States\")\\\n",
    "                        .withColumn('City', upper(col('City')))\n",
    "\n",
    "temperature_data=temperature_data.groupBy('City','month').agg({'AverageTemperature':'avg',\n",
    "                                                 'AverageTemperatureUncertainty':'avg'})\n",
    "\n",
    "temperature_data = temperature_data.select(\\\n",
    "                        col(\"City\").alias(\"city\"),\\\n",
    "                        \"month\",\\\n",
    "                        col(\"avg(AverageTemperature)\").alias(\"average_temperature\"),\\\n",
    "                        col(\"avg(AverageTemperatureUncertainty)\").alias(\"average_temperature_uncertainty\")).drop_duplicates()\n",
    "\n",
    "temperature_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immigration dataset cleaning\n",
    "\n",
    "In the Immigration dataset we will filter interesting columns and give them a more descriptabale name. Namely, we will consider fields that contain information about year, month, origin country, destination city, destination state, visa type, age, and gender fields We will filter only arrivals by air because we want to compare them with our airport dataset. If the origin country code or destination code is null we will consider that as invaluable data. Lastly, we will aggregate table by our subset of observed fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " cicid                 | 149.0    \n",
      " year                  | 2016     \n",
      " month                 | 1        \n",
      " origin_country_code   | 103      \n",
      " destination_city_code | TUC      \n",
      " destination_state     | NY       \n",
      " age                   | 35       \n",
      " gender                | F        \n",
      " visa_type             | WT       \n",
      " i94visa               | 2        \n",
      " count                 | 1        \n",
      " city_code             | TUC      \n",
      " city_name             | TUCSON   \n",
      " visa_purpose          | Pleasure \n",
      " origin_country        | AUSTRIA  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data = immigration_data.filter(immigration_data[\"i94mode\"]==1)\\\n",
    "                    .filter(immigration_data.i94res.isNotNull())\\\n",
    "                    .filter(immigration_data.i94port.isNotNull())\\\n",
    "                    .filter(immigration_data.i94visa.isNotNull())\\\n",
    "                    .withColumn(\"I94MON\",col(\"I94MON\").cast(\"integer\"))\\\n",
    "                    .withColumn(\"I94RES\",col(\"I94RES\").cast(\"integer\"))\\\n",
    "                    .withColumn(\"I94YR\",col(\"I94YR\").cast(\"integer\"))\\\n",
    "                    .withColumn(\"I94BIR\",col(\"I94BIR\").cast(\"integer\"))\\\n",
    "                    .withColumn(\"I94VISA\",col(\"I94VISA\").cast(\"integer\"))\\\n",
    "                    .withColumn(\"count\",col(\"count\").cast(\"integer\"))\n",
    "\n",
    "immigration_data = immigration_data.select(\n",
    "    \"cicid\",\n",
    "    col(\"I94YR\").alias(\"year\"),\n",
    "    col(\"I94MON\").alias(\"month\"),\n",
    "    col(\"I94RES\").alias(\"origin_country_code\"),\n",
    "    col(\"I94PORT\").alias(\"destination_city_code\"),\n",
    "    col(\"I94ADDR\").alias(\"destination_state\"),\n",
    "    col(\"I94BIR\").alias(\"age\"),\n",
    "    col(\"GENDER\").alias(\"gender\"),\n",
    "    col(\"visatype\").alias(\"visa_type\"),\n",
    "    col(\"I94VISA\").alias(\"i94visa\"),\n",
    "    \"count\").drop_duplicates()\n",
    "\n",
    "immigration_data = immigration_data.join(city_data, immigration_data.destination_city_code == city_data.city_code)\n",
    "\n",
    "immigration_data = immigration_data.withColumn('visa_purpose', i94visa_mapping(immigration_data[\"i94visa\"]))\n",
    "\n",
    "immigration_data = immigration_data.withColumn('origin_country', country_codes_mapping(immigration_data[\"origin_country_code\"]))\n",
    "\n",
    "immigration_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " year                  | 2016         \n",
      " month                 | 1            \n",
      " origin_country_code   | 261          \n",
      " origin_country        | SAUDI ARABIA \n",
      " destination_city_code | DAL          \n",
      " destination_city_name | DALLAS       \n",
      " destination_state     | TX           \n",
      " age                   | 29           \n",
      " gender                | F            \n",
      " visa_type             | F1           \n",
      " visa_purpose          | Student      \n",
      " count                 | 7            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data=immigration_data.groupBy(\n",
    "    \"year\",\\\n",
    "    \"month\",\\\n",
    "    \"origin_country_code\",\\\n",
    "    \"origin_country\",\\\n",
    "    \"destination_city_code\",\\\n",
    "    col(\"city_name\").alias(\"destination_city_name\"),\\\n",
    "    \"destination_state\",\\\n",
    "    \"age\",\\\n",
    "    \"gender\",\\\n",
    "    \"visa_type\",\\\n",
    "    \"visa_purpose\"\\\n",
    ").agg({\"count\":\"sum\"})\\\n",
    ".withColumnRenamed(\"sum(count)\",\"count\")\n",
    "\n",
    "immigration_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We will create a star schema with following dimension and fact tables.\n",
    "\n",
    "##### Dimension Tables\n",
    "\n",
    "Airport dimension ---> contains following information about airports:\n",
    "  * airport_id\n",
    "  * city_name\n",
    "  * state_code\n",
    "  * airport_name\n",
    "  * airport_type\n",
    "    \n",
    "Country dimension ---> contains following information about countries:\n",
    "  * country_name\n",
    "  * currency_name\n",
    "  * capital\n",
    "  * continent\n",
    "\n",
    "Demographics dimension ---> contains following information about demographics of U.S. cities:\n",
    "  * city_name\n",
    "  * state_code\n",
    "  * state_name\n",
    "  * total_population\n",
    "  * male_population\n",
    "  * female_population\n",
    "  * immigrants_population\n",
    "  * median_age\n",
    "\n",
    "Temperature dimension ---> contains following information about temperatures in U.S. cities:\n",
    "  * city\n",
    "  * month\n",
    "  * average_temperature\n",
    "  * average_temperature_uncertainty\n",
    "\n",
    "Immigration dimension ---> contains following information about immigration in U.S. cities:\n",
    "  * year\n",
    "  * month\n",
    "  * origin_country_code\n",
    "  * destination_city_code\n",
    "  * destination_state\n",
    "  * age\n",
    "  * gender\n",
    "  * visa_type\n",
    "  * i94visa\n",
    "  * destination_city\n",
    "  * origin_country\n",
    "  * visa_purpose\n",
    "  * count\n",
    "\n",
    "Fact Table will contain following data:\n",
    "  * year\n",
    "  * month\n",
    "  * destination_city_name\n",
    "  * destination_state\n",
    "  * number_of_airports\n",
    "  * total_population\n",
    "  * immigrants_population\n",
    "  * origin_country\n",
    "  * origin_continent\n",
    "  * origin_currency\n",
    "  * average_temperature\n",
    "  * visa_purpose\n",
    "  * count\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "We will generate the dimension tables from the transformed dataframes. Using Spark SQL we will join the dimension tables to create a fact table and write a fact table to parquet files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data.createOrReplaceTempView(\"airport_data\")\n",
    "country_data.createOrReplaceTempView(\"country_data\")\n",
    "demographics_data.createOrReplaceTempView(\"demographics_data\")\n",
    "temperature_data.createOrReplaceTempView(\"temperature_data\")\n",
    "immigration_data.createOrReplaceTempView(\"immigration_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " year                  | 2016              \n",
      " month                 | 1                 \n",
      " destination_city_name | MOBILE            \n",
      " destination_state     | Alabama           \n",
      " number_of_airports    | 6                 \n",
      " total_population      | 194305            \n",
      " immigrants_population | 7234              \n",
      " origin_country        | GREECE            \n",
      " origin_continent      | EU                \n",
      " origin_currency       | Euro              \n",
      " average_temperature   | 9.150419087136928 \n",
      " visa_purpose          | Business          \n",
      " count                 | 1                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    i.year, \n",
    "    i.month, \n",
    "    i.destination_city_name, \n",
    "    d.state_name as destination_state,\n",
    "    a.number_of_airports,\n",
    "    d.total_population,\n",
    "    d.immigrants_population,\n",
    "    i.origin_country,\n",
    "    c.continent as origin_continent,\n",
    "    c.currency_name as origin_currency,\n",
    "    t.average_temperature,\n",
    "    i.visa_purpose,\n",
    "    SUM(i.count) as count\n",
    "FROM immigration_data i\n",
    "JOIN country_data c ON i.origin_country = c.country_name\n",
    "JOIN demographics_data d ON i.destination_city_name = d.city_name\n",
    "JOIN temperature_data t ON i.destination_city_name = t.city AND i.month=t.month\n",
    "JOIN (\n",
    "    SELECT \n",
    "        state_code, \n",
    "        city_name, \n",
    "        COUNT(airport_id) as number_of_airports \n",
    "    FROM airport_data \n",
    "    GROUP BY \n",
    "    state_code, \n",
    "    city_name\n",
    "    ) a ON i.destination_city_name = a.city_name AND i.destination_state = a.state_code\n",
    "GROUP BY \n",
    "    i.year, \n",
    "    i.month, \n",
    "    i.destination_city_name, \n",
    "    d.state_name,\n",
    "    a.number_of_airports,\n",
    "    d.total_population,\n",
    "    d.immigrants_population,\n",
    "    i.origin_country,\n",
    "    c.continent,\n",
    "    c.currency_name,\n",
    "    t.average_temperature,\n",
    "    i.visa_purpose\n",
    "ORDER BY \n",
    "    d.state_name, \n",
    "    i.destination_city_name\n",
    "\"\"\")\n",
    "\n",
    "fact_table.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 9038 rows to parquet\n"
     ]
    }
   ],
   "source": [
    "# write fact table to parquet\n",
    "print('Writing {} rows to parquet'.format(fact_table.count()))\n",
    "fact_table.write.partitionBy('destination_state','destination_city_name').option('compression','snappy')\\\n",
    ".parquet(\"data_tables/fact_table\",mode='overwrite')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will check if any of the rows in the fact table contains null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n",
      " year                  | 0   \n",
      " month                 | 0   \n",
      " destination_city_name | 0   \n",
      " destination_state     | 0   \n",
      " number_of_airports    | 0   \n",
      " total_population      | 0   \n",
      " immigrants_population | 0   \n",
      " origin_country        | 0   \n",
      " origin_continent      | 0   \n",
      " origin_currency       | 0   \n",
      " average_temperature   | 0   \n",
      " visa_purpose          | 0   \n",
      " count                 | 0   \n",
      "\n",
      "-RECORD 0--------------------\n",
      " year                  | 0   \n",
      " month                 | 0   \n",
      " destination_city_name | 0   \n",
      " destination_state     | 0   \n",
      " number_of_airports    | 0   \n",
      " total_population      | 0   \n",
      " immigrants_population | 0   \n",
      " origin_country        | 0   \n",
      " origin_continent      | 0   \n",
      " origin_currency       | 0   \n",
      " average_temperature   | 0   \n",
      " visa_purpose          | 0   \n",
      " count                 | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.select([count(when(isnan(column), column)).alias(column) for column in fact_table.columns]).show(vertical=True)\n",
    "fact_table.select([count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in fact_table.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will check if the rows have been populated and what is the shape of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 9038\n",
      "Columns: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: {r}\\nColumns: {c}\".format(\n",
    "    r=fact_table.count(),c=len(fact_table.columns)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check if we had filtered the data correctly so the aggregations on the fact table are lower than immigration data aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------\n",
      " sum(count) | 1050485 \n",
      "\n",
      "-RECORD 0-------------\n",
      " sum(count) | 2761287 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.agg({\"count\":\"sum\"}).show(1,vertical=True)\n",
    "immigration_data.agg({\"count\":\"sum\"}).show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we will check if we had succesfully added currencies data to the dataset, as we had some null values there before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table.filter(fact_table[\"origin_currency\"].isNull()).groupBy('origin_country').agg({'count':'sum'}).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we were careful about data quality in the whole process of setting up this pipeline, as we had done some basic checks before, and always tried to show at least one row of our data at each step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### airport_data - data was extracted from airport data downloaded from Datahub\n",
    "  * airport_id(string) - Unique airport ID code\n",
    "  * city_name(string) - City airport is located in\n",
    "  * state_code(string) - Abbriviation of state airport is located in\n",
    "  * airport_name(string) - Airport name\n",
    "  * airport_type(string) - Airport type\n",
    "\n",
    "##### country_data - data was extracted from country data downloaded from Datahub\n",
    "  * country_name(string) - Country name\n",
    "  * currency_name(string) - Country currency\n",
    "  * capital(string) - Country capital\n",
    "  * continent(string) - Continent country is located in\n",
    "\n",
    "##### demographics_data - data was extracted from demographics data downloaded from OpenSoft\n",
    "  * city_name(string) - City name\n",
    "  * state_code(string) - Abbriviation of state\n",
    "  * state_name(string) - State name\n",
    "  * total_population(integer) - Total population of the city\n",
    "  * male_population(integer) - Male population of the city\n",
    "  * female_population(integer) - Female population of the city\n",
    "  * immigrants_population(integer) - Immigrants population of the city\n",
    "  * median_age(float) - Median age of the population\n",
    "\n",
    "##### temperature_data - data was extracted from temperature data downloaded from Kaggle\n",
    "  * city(string) - City name\n",
    "  * month(integer) - Month\n",
    "  * average_temperature(double) - Average temperature in degrees Celsius\n",
    "  * average_temperature_uncertainty(double) - Average temperature uncertanty in degrees Celsius\n",
    "\n",
    "\n",
    "##### immigration_data - data was extracted from original I94 Immigrations data\n",
    "  * year(integer) - Year\n",
    "  * month(integer) - Month\n",
    "  * origin_country_code(integer) - Country of origin code\n",
    "  * origin_country(string) - Country of origin name calculated from origin_country_code using SAS Labels Descriptions\n",
    "  * destination_city_code(string) - Destination city code\n",
    "  * destination_city_name(string) - Destination city name calculated from destination_city_code using SAS Labels Descriptions\n",
    "  * destination_state(string) - Abbriviation of destination state\n",
    "  * age(integer) - Age of traveler listed on I94\n",
    "  * gender(string) - Gender of traveler listed on I94 \n",
    "  * visa_type(string) - Visa type\n",
    "  * visa_purpose(string) - Visa purpose calculated from origin code using SAS Labels Descriptions\n",
    "  * count(long) Number of arrivals\n",
    "\n",
    "\n",
    "##### fact_table \n",
    "  * year(integer) - Year\n",
    "  * month(integer) - Month\n",
    "  * destination_city_name(string) - Destination city name\n",
    "  * destination_state(string) - Destination state name\n",
    "  * number_of_airports(long) - Number of airports in the city\n",
    "  * total_population(integer) - Total population of the city\n",
    "  * immigrants_population(integer) - Immigrants population of the city\n",
    "  * origin_country(string) - Country of origin name\n",
    "  * origin_continent(string) - Continent of origin\n",
    "  * origin_currency(string) - Currency in country of origin\n",
    "  * average_temperature(double) - Average temperature in degrees Celsius\n",
    "  * visa_purpose(string) - Visa purpose (1-Business 2-Pleasure 3-Student)\n",
    "  * count(long) - Number of arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "We had used Python and Apache Spark for their speed and usabillity on this smaller dataset. These tools contained all necesary libraries we used to extract, read, clean, process and store tables. Due to limited dataset available, we had locally stored the fact table using the Spark SQL in parquet files partitioned by city and state. The data could be updated at monthly basis as that is level of aggregation we used here, and the level of aggregation the imigration data is provided.\n",
    "\n",
    "If the data would increase by 100 times, we would store the input data in the cloud storage AWS S3. To process the data clustered Spark would be used as it would allow parallel processing of the data. We would consider using AWS Redshift to store the staging and the final tables, while the output data would be stored bask to AWS S3.\n",
    "\n",
    "If the data populates a dashboard that must be updated on a daily basis by 7am every day, we would use Airflow to schedule and run the data pipeline. \n",
    "\n",
    "If the database needed to be accessed by 100+ people, we could replicate the data to different nodes used by different users. Moreover, we could store the data in the AWS and use the web app to access the data. If the usage would increase further more we could build a dashboard using some BI tool, e.g. Tableu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
